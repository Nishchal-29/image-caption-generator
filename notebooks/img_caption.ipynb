{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6smui2uPmL2q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, RepeatVector, Reshape, concatenate\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "os.environ['TQDM_DISABLE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_631HwUn1kd",
    "outputId": "462934dd-98e4-432f-a7a2-d394753a3232"
   },
   "outputs": [],
   "source": [
    "# Mapping image IDs to their descriptions\n",
    "\n",
    "def load_doc(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    text = f.read()\n",
    "  return text\n",
    "\n",
    "filename='Flickr8k.token.txt'\n",
    "doc=load_doc(filename)\n",
    "\n",
    "desc = {}\n",
    "for line in doc.strip().split('\\n'):\n",
    "  tokens = line.split('\\t')\n",
    "  img_id, img_desc = tokens[0], tokens[1]\n",
    "  img_id = img_id.split('#')[0]\n",
    "  img_desc = img_desc.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "  if img_id not in desc:\n",
    "    desc[img_id] = []\n",
    "  desc[img_id].append('startseq'+img_desc+'endseq')\n",
    "\n",
    "print(f\"Loaded {len(desc)} image descriptions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "aa18d4d68cb6442696915c147f2bad3a",
      "7c0204396f7b458abfb3bf6456b6aad2",
      "97ea8b019ead4c75be418148e2ac7d90",
      "9979bd0f015a4f0f93aefd06d8cc25c6",
      "fd8c50591a854c649ac6a479cb3433a1",
      "927e835db7a44c74a9c8149991fabea8",
      "cff909dc23a548239ac0bde5a3118ac8",
      "3ef0285af044415b9c92581f49ae69a6",
      "518e94f6502243a7841aa51a770eaee5",
      "956fcfec28e94e4ea12598fd6fad6ad7",
      "028bbc580b7a4c89ba8f29cb43c6db48"
     ]
    },
    "id": "d6ZBLslHqAuH",
    "outputId": "b47c76d8-a846-4bef-fd7f-66fb6e1974d6"
   },
   "outputs": [],
   "source": [
    "# Extracting features from images using InceptionV3 pre-trained model\n",
    "\n",
    "model = InceptionV3(weights='imagenet')\n",
    "model = Model(model.input, model.layers[-2].output)\n",
    "\n",
    "features = {}\n",
    "images = glob.glob('Flicker8k_Dataset/*.jpg')\n",
    "\n",
    "for img_path in tqdm(images, disable=False):\n",
    "  img_id = os.path.basename(img_path).split('.')[0]\n",
    "  img = image.load_img(img_path, target_size=(299, 299))\n",
    "  img = image.img_to_array(img)\n",
    "  img = np.expand_dims(img, axis=0)\n",
    "  img = preprocess_input(img)\n",
    "  feature = model.predict(img, verbose=0)\n",
    "  features[img_id] = feature\n",
    "\n",
    "print(f\"Extracted features for {len(features)} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YzSbDdyrmJ2",
    "outputId": "f20fc6fe-5e67-4a3d-9076-f39a35e1a772"
   },
   "outputs": [],
   "source": [
    "# Preparing the tokenizer and vocabulary size\n",
    "\n",
    "descs = []\n",
    "for key in desc:\n",
    "  [descs.append(d) for d in desc[key]]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(descs)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxLen = max(len(d.split()) for d in descs)\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Max Caption Length: {maxLen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYkE9pNmtL2C"
   },
   "outputs": [],
   "source": [
    "# Function to create a TensorFlow dataset from the image descriptions and features\n",
    "\n",
    "def create_tf_dataset(desc, features, tokenizer, maxLen, vocab_size, batch_size):\n",
    "    def data_generator():\n",
    "        while True:\n",
    "            for key, value in desc.items():\n",
    "                if key in features:\n",
    "                    feature = features[key][0]\n",
    "                    for d in value:\n",
    "                        seq = tokenizer.texts_to_sequences([d])[0]\n",
    "                        for i in range(1, len(seq)):\n",
    "                            in_seq, out_seq = seq[:i], seq[i]\n",
    "                            in_seq = pad_sequences([in_seq], maxlen=maxLen)[0]\n",
    "                            out_seq_onehot = to_categorical(out_seq, num_classes=vocab_size)\n",
    "                            yield (feature, in_seq), out_seq_onehot\n",
    "    feature_dim = features[list(features.keys())[0]][0].shape[0]\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(feature_dim,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(maxLen,), dtype=tf.int32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "    dataset = tf.data.Dataset.from_generator(data_generator, output_signature=output_signature)\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "snKOZphdvs62",
    "outputId": "f4d9925f-e5c9-4a48-db8c-f752cf1e9eee"
   },
   "outputs": [],
   "source": [
    "# Architecture taken from the paper \"Show and Tell: A Neural Image Caption Generator\" -> https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "inputs1 = Input(shape=(2048,))\n",
    "feature1 = Dense(256, activation='relu')(inputs1)\n",
    "feature1_reshaped = Reshape((1, 256), input_shape=(256,))(feature1)\n",
    "\n",
    "inputs2 = Input(shape=(maxLen,))\n",
    "emb1 = Embedding(vocab_size, 256, mask_zero = False)(inputs2)\n",
    "merged = concatenate([feature1_reshaped, emb1], axis=1)\n",
    "emb2 = LSTM(256)(merged)\n",
    "emb3 = Dropout(0.5)(emb2)\n",
    "combined = add([emb3, feature1])\n",
    "x = Dense(256, activation='relu')(combined)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(vocab_size, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBK-JDPtynS0"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "def normalize_desc_keys(desc):\n",
    "    normalized_desc = {}\n",
    "    for key, value in desc.items():\n",
    "        new_key = key.replace('.jpg', '')\n",
    "        normalized_desc[new_key] = value\n",
    "    return normalized_desc\n",
    "\n",
    "desc_normalized = normalize_desc_keys(desc)\n",
    "\n",
    "common_keys = set(desc_normalized.keys()).intersection(set(features.keys()))\n",
    "filtered_desc = {key: desc_normalized[key] for key in common_keys}\n",
    "\n",
    "def calculate_steps(desc, batch_size):\n",
    "    total_seq = 0\n",
    "    for key, value in desc.items():\n",
    "        for d in value:\n",
    "            seq_len = len(tokenizer.texts_to_sequences([d])[0])\n",
    "            total_seq += max(1, seq_len - 1)\n",
    "    return total_seq // batch_size\n",
    "\n",
    "steps = calculate_steps(filtered_desc, batch_size)\n",
    "print(f\"Total Steps per Epoch: {steps}\")\n",
    "print(f\"Number of images: {len(filtered_desc)}\")\n",
    "print(f\"Total sequences: {steps * batch_size}\")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint('model.h5', save_best_only=True, monitor='loss'),\n",
    "    EarlyStopping(patience=5, monitor='loss', restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-7)\n",
    "]\n",
    "\n",
    "dataset = create_tf_dataset(filtered_desc, features, tokenizer, maxLen, vocab_size, batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    epochs=epochs,\n",
    "    steps_per_epoch=steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TYZ4C4lKz8NJ"
   },
   "outputs": [],
   "source": [
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = None\n",
    "        for w, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                word = w\n",
    "                break\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "keys = list(features.keys())\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image_file_path = 'Flicker8k_Dataset/' + keys[19] + '.jpg'\n",
    "img = mpimg.imread(image_file_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "photo = features[keys[19]]\n",
    "print(generate_desc(model, tokenizer, photo, maxLen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_word(integer,tokenizer):\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "def predict_caption(model, image, tokenizer, max_length, features):\n",
    "    \n",
    "    feature = features[image]\n",
    "    in_text = \"startseq\"\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], max_length)\n",
    "\n",
    "        y_pred = model.predict([feature,sequence])\n",
    "        y_pred = np.argmax(y_pred)\n",
    "        \n",
    "        word = idx_to_word(y_pred, tokenizer)\n",
    "        \n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        in_text+= \" \" + word\n",
    "        \n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "    return in_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['Flicker8k_Dataset/1000268201_693b08cb0e.jpg', 'Flicker8k_Dataset/1001773457_577c3a7d70.jpg', 'Flicker8k_Dataset/1002674143_1b742ab4b8.jpg']\n",
    "for sample in samples:\n",
    "    img = mpimg.imread(sample)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    img_id = os.path.basename(sample).split('.')[0]\n",
    "    caption = predict_caption(model, img_id, tokenizer, maxLen, features)\n",
    "    print(f\"Predicted Caption: {caption}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples:\n",
    "    img_id = os.path.basename(sample).split('.')[0]\n",
    "    print(f\"{img_id} feature norm: {np.linalg.norm(features[img_id])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
