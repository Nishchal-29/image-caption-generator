{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6smui2uPmL2q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_631HwUn1kd",
    "outputId": "462934dd-98e4-432f-a7a2-d394753a3232"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class Vocabulary:\n",
    "  def __init__(self, freq_threshold):\n",
    "    self.freq_threshold = freq_threshold\n",
    "    self.word2idx = {\"<pad>\" : 0, \"<start>\" : 1, \"<end>\" : 2, \"<unk>\" : 3}\n",
    "    self.idx2word = {0 : \"<pad>\", 1 : \"<start>\", 2 : \"<end>\", 3 : \"<unk>\"}\n",
    "    self.idx = 4\n",
    "    \n",
    "  def tokenizer(self, text):\n",
    "    return [token.text.lower() for token in nlp(text)]\n",
    "  \n",
    "  def build_vocab(self, sentence_list):\n",
    "    frequencies = Counter()\n",
    "    for sentence in sentence_list:\n",
    "      frequencies.update(self.tokenizer(sentence))\n",
    "      \n",
    "    for word, freq in frequencies.items():\n",
    "      if freq >= self.freq_threshold:\n",
    "        self.word2idx[word] = self.idx\n",
    "        self.idx2word[self.idx] = word\n",
    "        self.idx += 1\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "      tokenized_text = self.tokenizer(text)\n",
    "      return [self.word2idx.get(token, self.word2idx[\"<unk>\"]) for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "aa18d4d68cb6442696915c147f2bad3a",
      "7c0204396f7b458abfb3bf6456b6aad2",
      "97ea8b019ead4c75be418148e2ac7d90",
      "9979bd0f015a4f0f93aefd06d8cc25c6",
      "fd8c50591a854c649ac6a479cb3433a1",
      "927e835db7a44c74a9c8149991fabea8",
      "cff909dc23a548239ac0bde5a3118ac8",
      "3ef0285af044415b9c92581f49ae69a6",
      "518e94f6502243a7841aa51a770eaee5",
      "956fcfec28e94e4ea12598fd6fad6ad7",
      "028bbc580b7a4c89ba8f29cb43c6db48"
     ]
    },
    "id": "d6ZBLslHqAuH",
    "outputId": "b47c76d8-a846-4bef-fd7f-66fb6e1974d6"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  def __init__(self, image_dir, caption_file, vocab, transform, mode=\"train\"):\n",
    "    self.image_dir = image_dir\n",
    "    self.df = pd.read_csv(caption_file)\n",
    "    self.images = self.df['image'].tolist()\n",
    "    self.captions = self.df['caption'].tolist()\n",
    "    self.vocab = vocab\n",
    "    self.transform = transform\n",
    "    self.mode = mode\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = self.transform(img)\n",
    "    \n",
    "    if self.mode == \"train\":\n",
    "      caption = self.captions[idx]\n",
    "      caption += self.vocab.numericalize(self.captions[idx])\n",
    "      caption.append(self.vocab.word2idx[\"<end>\"])\n",
    "      return img, np.array(caption, dtype=np.int32)\n",
    "    else:\n",
    "      return img, self.images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YzSbDdyrmJ2",
    "outputId": "f20fc6fe-5e67-4a3d-9076-f39a35e1a772"
   },
   "outputs": [],
   "source": [
    "def transform_img(img):\n",
    "  img = img.resize((224, 224))\n",
    "  img = np.array(img)\n",
    "  img = preprocess_input(img)\n",
    "  return img\n",
    "\n",
    "def data_generator(dataset):\n",
    "  for i in range(len(dataset)):\n",
    "      img, cap = dataset[i]\n",
    "      print(f\"Yielding item {i}\")\n",
    "      yield img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KYkE9pNmtL2C"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.cnn = ResNet50(include_top=False, weights='imagenet')\n",
    "        self.pool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(embedding_dim)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "snKOZphdvs62",
    "outputId": "f4d9925f-e5c9-4a48-db8c-f752cf1e9eee"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "        \n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_wts = tf.nn.softmax(self.V(score), axis = 1)\n",
    "        context_vector = attention_wts * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        return context_vector, attention_wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xBK-JDPtynS0"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(units=units)\n",
    "        \n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context_vector, attention_wts = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state = [hidden, cell])\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state_h, state_c, attention_wts\n",
    "    \n",
    "    def reset_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units)), tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(img_tensor, target, encoder, decoder, optimizer, vocab):\n",
    "    loss = 0\n",
    "    hidden, cell = decoder.reset_state(batch_size = target.shape[0])\n",
    "    dec_input = tf.expand_dims([vocab.word2idx['<start>']] * target.shape[0], 1)\n",
    "    features = encoder(img_tensor)\n",
    "    \n",
    "    for i in range(1, target.shape[1]):\n",
    "        predictions, hidden, cell, attention_wts = decoder(dec_input, features, hidden, cell)\n",
    "        loss += loss_function(target[:, i], predictions)\n",
    "        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = loss / int(target.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tf.gradients(total_loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(img, encoder, decoder, vocab, beam_width=3, maxLen=50):\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    features = encoder(img)\n",
    "    hidden, cell = decoder.reset_state(batch_size=1)\n",
    "    \n",
    "    start_token = vocab.word2idx['<start>']\n",
    "    end_token = vocab.word2idx['<end>']\n",
    "    beams = [([start_token], hidden, cell, 0.0)] \n",
    "    \n",
    "    for _ in range(maxLen):\n",
    "        candidates = []\n",
    "        for seq, h, c, score in beams:\n",
    "            if seq[-1] == end_token:\n",
    "                candidates.append((seq, h, c, score))\n",
    "                continue\n",
    "            \n",
    "            dec_input = tf.expand_dims([seq[-1]], 0)\n",
    "            predictions, h_new, c_new, _ = decoder(dec_input, features, h, c)\n",
    "            predictions = tf.nn.softmax(predictions, axis = -1)\n",
    "            top_k_probs, top_k_indices = tf.math.top_k(predictions, k=beam_width)\n",
    "            \n",
    "            for i in range(beam_width):\n",
    "                word_idx = top_k_indices[0][i].numpy()\n",
    "                word_log_prob = top_k_probs[0][i].numpy()\n",
    "                candidates.append((seq + [word_idx], h_new, c_new, score + word_log_prob))\n",
    "                \n",
    "                \n",
    "        ordered = sorted(candidates, key = lambda tup : tup[3] / len(tup[0]), reverse=True)\n",
    "        beams = ordered[:beam_width]\n",
    "        \n",
    "        if all(seq[-1] == end_token for seq, _, _, _ in beams):\n",
    "            break\n",
    "        best_seq = beams[0][0]\n",
    "        caption = [vocab.idx2word[idx] for idx in best_seq if idx not in [start_token, end_token, vocab.word2idx['<pad>']]]\n",
    "        return ' '.join(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_model(encoder, decoder, vocab):\n",
    "    encoder.save(\"encoder_model\")\n",
    "    decoder.save_weights(\"decoder_weights.h5\")\n",
    "\n",
    "    with open(\"vocab.pkl\", \"wb\") as f:\n",
    "        pickle.dump(vocab, f)\n",
    "        \n",
    "def load_model_for_inference(vocab_dim, embed_dim=256, units=512):\n",
    "    encoder = tf.keras.models.load_model(\"encoder_model\", custom_objects={\"EncoderCNN\": EncoderCNN})\n",
    "    decoder = DecoderRNN(vocab_dim, embedding_dim=embed_dim, units=units)\n",
    "    decoder.build(input_shape=(None, 1))\n",
    "    decoder.load_weights(\"decoder_weights.h5\")\n",
    "\n",
    "    import pickle\n",
    "    with open(\"vocab.pkl\", \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    return encoder, decoder, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]2025-07-28 00:29:04.463035: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: NameError: name 'Image' is not defined\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/tmp/ipykernel_113036/1623902149.py\", line 9, in data_generator\n",
      "    img, cap = dataset[i]\n",
      "               ~~~~~~~^^^\n",
      "\n",
      "  File \"/tmp/ipykernel_113036/3396009401.py\", line 16, in __getitem__\n",
      "    img = Image.open(img_path).convert(\"RGB\")\n",
      "          ^^^^^\n",
      "\n",
      "NameError: name 'Image' is not defined\n",
      "\n",
      "\n",
      "2025-07-28 00:29:04.463506: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: NameError: name 'Image' is not defined\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n",
      "    ret = func(*args)\n",
      "          ^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"/tmp/ipykernel_113036/1623902149.py\", line 9, in data_generator\n",
      "    img, cap = dataset[i]\n",
      "               ~~~~~~~^^^\n",
      "\n",
      "  File \"/tmp/ipykernel_113036/3396009401.py\", line 16, in __getitem__\n",
      "    img = Image.open(img_path).convert(\"RGB\")\n",
      "          ^^^^^\n",
      "\n",
      "NameError: name 'Image' is not defined\n",
      "\n",
      "\n",
      "\t [[{{node PyFunc}}]]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'Image' is not defined\nTraceback (most recent call last):\n\n  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/tmp/ipykernel_113036/1623902149.py\", line 9, in data_generator\n    img, cap = dataset[i]\n               ~~~~~~~^^^\n\n  File \"/tmp/ipykernel_113036/3396009401.py\", line 16, in __getitem__\n    img = Image.open(img_path).convert(\"RGB\")\n          ^^^^^\n\nNameError: name 'Image' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnknownError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m EPOCHS = \u001b[32m5\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.numpy()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents-env/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:826\u001b[39m, in \u001b[36mOwnedIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    825\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m826\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    827\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m errors.OutOfRangeError:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/data/ops/iterator_ops.py:776\u001b[39m, in \u001b[36mOwnedIterator._next_internal\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[32m    774\u001b[39m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[32m    775\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context.execution_mode(context.SYNC):\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m   ret = \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    781\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    782\u001b[39m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._element_spec._from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001b[39m, in \u001b[36miterator_get_next\u001b[39m\u001b[34m(iterator, output_types, output_shapes, name)\u001b[39m\n\u001b[32m   3084\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   3085\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3086\u001b[39m   \u001b[43m_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3087\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _core._FallbackException:\n\u001b[32m   3088\u001b[39m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:6006\u001b[39m, in \u001b[36mraise_from_not_ok_status\u001b[39m\u001b[34m(e, name)\u001b[39m\n\u001b[32m   6004\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mraise_from_not_ok_status\u001b[39m(e, name) -> NoReturn:\n\u001b[32m   6005\u001b[39m   e.message += (\u001b[33m\"\u001b[39m\u001b[33m name: \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m6006\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m core._status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mUnknownError\u001b[39m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} NameError: name 'Image' is not defined\nTraceback (most recent call last):\n\n  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/home/devcontainers/miniconda3/envs/agents-env/lib/python3.11/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/tmp/ipykernel_113036/1623902149.py\", line 9, in data_generator\n    img, cap = dataset[i]\n               ~~~~~~~^^^\n\n  File \"/tmp/ipykernel_113036/3396009401.py\", line 16, in __getitem__\n    img = Image.open(img_path).convert(\"RGB\")\n          ^^^^^\n\nNameError: name 'Image' is not defined\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "root_folder = \"/home/devcontainers/Datasets/Images\"\n",
    "caption_file = \"/home/devcontainers/Datasets/captions.txt\"\n",
    "\n",
    "vocab = Vocabulary(freq_threshold=5)\n",
    "dataset = Dataset(root_folder, caption_file, vocab, transform_img)\n",
    "vocab.build_vocab(dataset.captions)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(dataset),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(224, 224, 3), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ").padded_batch(32, padded_shapes=([224, 224, 3], [None]))\n",
    "\n",
    "# Init Model\n",
    "encoder = EncoderCNN(embedding_dim=256)\n",
    "decoder = DecoderRNN(len(vocab.word2idx), embedding_dim=256, units=512)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Train\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    for (img_tensor, target) in tqdm(train_ds):\n",
    "        loss = train(img_tensor, target, encoder, decoder, optimizer, vocab)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# Save model\n",
    "save_model(encoder, decoder, vocab)\n",
    "\n",
    "# Inference\n",
    "encoder, decoder, vocab = load_model_for_inference(len(vocab.word2idx))\n",
    "\n",
    "# Load test image\n",
    "test_img_path = os.path.join(root_folder, dataset.images[0])\n",
    "test_img = transform_img(Image.open(test_img_path).convert(\"RGB\"))\n",
    "\n",
    "# Generate caption\n",
    "print(\"Generated Caption:\", beam_search(test_img, encoder, decoder, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agents-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
